#!/usr/bin/env python3
"""
Insight Script

ä»å­¦ä¹ è®°å½•ä¸­æå–çµæ„Ÿï¼Œç”Ÿæˆé€‚ç”¨äºç¤¾äº¤åª’ä½“åˆ›ä½œçš„å†…å®¹å»ºè®®ã€‚
åˆ†æå­¦ä¹ å†…å®¹çš„ä¸»é¢˜è¶‹åŠ¿ã€äº§å“ä½“éªŒã€æŠ€æœ¯æ´å¯Ÿï¼Œä¸ºå†…å®¹åˆ›ä½œæä¾›ç´ æå’Œçµæ„Ÿã€‚
"""

import os
import sys
import json
import argparse
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict, Counter
import tempfile
import subprocess


class InsightAnalyzer:
    def __init__(self):
        self.base_dir = Path.cwd()
        self.weeks_dir = self.base_dir / "weeks"

    def get_analysis_date_range(self, days: Optional[int] = None,
                              weeks: Optional[int] = None,
                              all_content: bool = False) -> Tuple[datetime, datetime]:
        """Calculate the date range for analysis."""
        end_date = datetime.now()

        if all_content:
            # Find earliest file
            daily_files = list(self.weeks_dir.glob("*/????_??_??.md"))
            if daily_files:
                daily_files.sort(key=lambda x: x.name)
                earliest_file = daily_files[0]
                try:
                    filename = earliest_file.stem
                    year, month, day = map(int, filename.split('_'))
                    start_date = datetime(year, month, day)
                except (ValueError, IndexError):
                    start_date = datetime(2025, 9, 15)
            else:
                start_date = datetime(2025, 9, 15)
        elif weeks:
            start_date = end_date - timedelta(weeks=weeks)
        else:
            # Default to days (7 if not specified)
            days = days or 7
            start_date = end_date - timedelta(days=days)

        return start_date, end_date

    def collect_content_in_range(self, start_date: datetime, end_date: datetime,
                                topic: Optional[str] = None) -> Dict[str, List[Any]]:
        """Collect all content within the specified date range."""
        content = {
            'videos': [],
            'readings': [],
            'thoughts': [],
            'outputs': [],
            'projects': [],
            'insights': [],
            'product_experiences': [],
            'technical_notes': []
        }

        daily_files = list(self.weeks_dir.glob("*/????_??_??.md"))

        for file_path in daily_files:
            try:
                # Parse file date
                filename = file_path.stem
                year, month, day = map(int, filename.split('_'))
                file_date = datetime(year, month, day)

                # Skip files outside date range
                if file_date < start_date or file_date > end_date:
                    continue

                # Read and parse file content
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()

                # Apply topic filter if specified
                if topic and not self._content_matches_topic(file_content, topic):
                    continue

                date_str = file_date.strftime('%Y-%m-%d')

                # Extract video content
                video_section = self._extract_section(file_content, 'video')
                if video_section.strip():
                    videos = self._parse_video_content(video_section, date_str)
                    content['videos'].extend(videos)

                # Extract reading materials
                newsletter_section = self._extract_section(file_content, 'newsletter')
                if newsletter_section.strip():
                    readings = self._parse_reading_content(newsletter_section, date_str)
                    content['readings'].extend(readings)

                # Extract thoughts and braindump
                braindump_section = self._extract_section(file_content, 'braindump')
                if braindump_section.strip():
                    thoughts, insights, products = self._parse_braindump_content(braindump_section, date_str)
                    content['thoughts'].extend(thoughts)
                    content['insights'].extend(insights)
                    content['product_experiences'].extend(products)

                # Extract outputs
                output_section = self._extract_section(file_content, 'output')
                if output_section.strip():
                    outputs = self._parse_output_content(output_section, date_str)
                    content['outputs'].extend(outputs)

                # Extract project progress
                waytoace_section = self._extract_section(file_content, 'WayToAce')
                if waytoace_section.strip():
                    content['projects'].append({
                        'date': date_str,
                        'content': waytoace_section.strip(),
                        'type': 'WayToAce'
                    })

            except Exception as e:
                print(f"Error processing file {file_path}: {e}")
                continue

        return content

    def _extract_section(self, content: str, section: str) -> str:
        """Extract content from a specific markdown section."""
        pattern = rf"^## {re.escape(section)}$(.*?)(?=^## |\Z)"
        match = re.search(pattern, content, re.MULTILINE | re.DOTALL)
        return match.group(1).strip() if match else ""

    def _content_matches_topic(self, content: str, topic: str) -> bool:
        """Check if content matches the specified topic."""
        topic_lower = topic.lower()
        content_lower = content.lower()
        return topic_lower in content_lower

    def _parse_video_content(self, content: str, date: str) -> List[Dict[str, Any]]:
        """Parse video learning content."""
        videos = []
        lines = content.split('\n')

        current_video = None
        for line in lines:
            line = line.strip()
            if line.startswith('- [') and '](' in line:
                # Extract video title and URL
                match = re.match(r'- \[(.*?)\]\((.*?)\)', line)
                if match:
                    title, url = match.groups()
                    current_video = {
                        'date': date,
                        'title': title,
                        'url': url,
                        'notes': []
                    }
                    videos.append(current_video)
            elif line.startswith('  ') and current_video:
                # Add notes to current video
                current_video['notes'].append(line.strip())

        return videos

    def _parse_reading_content(self, content: str, date: str) -> List[Dict[str, Any]]:
        """Parse reading materials content."""
        readings = []
        for line in content.split('\n'):
            line = line.strip()
            if '[x]' in line.lower():
                # Extract completed reading item
                reading_text = re.sub(r'\[x\]', '', line, flags=re.IGNORECASE).strip()
                if reading_text:
                    readings.append({
                        'date': date,
                        'content': reading_text,
                        'completed': True
                    })

        return readings

    def _parse_braindump_content(self, content: str, date: str) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """Parse braindump content into thoughts, insights, and product experiences."""
        thoughts = []
        insights = []
        products = []

        lines = content.split('\n')
        current_item = None
        current_type = 'thought'

        for line in lines:
            line = line.strip()

            # Detect insights sections
            if 'insights:' in line.lower() or 'insight:' in line.lower():
                current_type = 'insight'
                continue

            # Detect product experience sections
            if any(keyword in line.lower() for keyword in ['äº§å“ä½¿ç”¨ä½“éªŒ', 'ä½“éªŒäº†', 'è¯•ç”¨äº†']):
                current_type = 'product'
                current_item = {
                    'date': date,
                    'content': line,
                    'details': []
                }
                products.append(current_item)
                continue

            if line.startswith('- '):
                # New main point
                item_content = line[2:].strip()
                current_item = {
                    'date': date,
                    'content': item_content,
                    'details': []
                }

                if current_type == 'insight':
                    insights.append(current_item)
                elif current_type == 'product':
                    products.append(current_item)
                else:
                    thoughts.append(current_item)

            elif line.startswith('  ') and current_item:
                # Add detail to current item
                current_item['details'].append(line.strip())

        return thoughts, insights, products

    def _parse_output_content(self, content: str, date: str) -> List[Dict[str, Any]]:
        """Parse learning output content."""
        outputs = []
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('- '):
                output_text = line[2:].strip()

                # Try to extract URL if present
                url_match = re.search(r'\[(.*?)\]\((.*?)\)', output_text)
                if url_match:
                    title, url = url_match.groups()
                    outputs.append({
                        'date': date,
                        'title': title,
                        'url': url,
                        'type': 'link'
                    })
                else:
                    outputs.append({
                        'date': date,
                        'content': output_text,
                        'type': 'text'
                    })

        return outputs

    def analyze_themes_and_trends(self, content: Dict[str, List[Any]]) -> Dict[str, Any]:
        """Analyze themes and trends from collected content."""
        analysis = {
            'top_keywords': [],
            'themes': {},
            'trends': [],
            'content_summary': {}
        }

        # Collect all text content for keyword analysis
        all_text = []

        # Add video content
        for video in content['videos']:
            all_text.append(video['title'])
            all_text.extend(video['notes'])

        # Add thoughts and insights
        for thought in content['thoughts']:
            all_text.append(thought['content'])
            all_text.extend(thought['details'])

        for insight in content['insights']:
            all_text.append(insight['content'])
            all_text.extend(insight['details'])

        # Add product experiences
        for product in content['product_experiences']:
            all_text.append(product['content'])
            all_text.extend(product['details'])

        # Extract keywords (simplified approach)
        text_content = ' '.join(all_text).lower()

        # Define important keywords to look for
        tech_keywords = ['ai', 'claude', 'gpt', 'python', 'javascript', 'react', 'vue', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'äººå·¥æ™ºèƒ½']
        product_keywords = ['äº§å“', 'ç”¨æˆ·ä½“éªŒ', 'ux', 'ui', 'è®¾è®¡', 'åŠŸèƒ½', 'éœ€æ±‚', 'è¿­ä»£']
        business_keywords = ['å•†ä¸š', 'å˜ç°', 'å•†ä¸šåŒ–', 'åˆ›ä¸š', 'æŠ•èµ„', 'å¸‚åœº', 'ç”¨æˆ·', 'è·å®¢']

        # Count keyword frequencies
        keyword_counts = Counter()
        for keyword in tech_keywords + product_keywords + business_keywords:
            count = text_content.count(keyword)
            if count > 0:
                keyword_counts[keyword] = count

        analysis['top_keywords'] = keyword_counts.most_common(10)

        # Categorize themes
        analysis['themes'] = {
            'technology': [kw for kw, _ in keyword_counts.most_common() if kw in tech_keywords],
            'product': [kw for kw, _ in keyword_counts.most_common() if kw in product_keywords],
            'business': [kw for kw, _ in keyword_counts.most_common() if kw in business_keywords]
        }

        # Content summary
        analysis['content_summary'] = {
            'total_videos': len(content['videos']),
            'total_readings': len(content['readings']),
            'total_thoughts': len(content['thoughts']),
            'total_insights': len(content['insights']),
            'total_outputs': len(content['outputs']),
            'total_products': len(content['product_experiences']),
            'total_projects': len(content['projects'])
        }

        return analysis

    def generate_content_suggestions(self, content: Dict[str, List[Any]],
                                   analysis: Dict[str, Any],
                                   platform: Optional[str] = None) -> List[Dict[str, Any]]:
        """Generate content creation suggestions based on analysis."""
        suggestions = []

        # Platform-specific templates
        platform_templates = {
            'xiaohongshu': {
                'max_length': 1000,
                'style': 'casual',
                'hashtags': True,
                'emojis': True
            },
            'weibo': {
                'max_length': 140,
                'style': 'concise',
                'hashtags': True,
                'emojis': False
            },
            'linkedin': {
                'max_length': 3000,
                'style': 'professional',
                'hashtags': False,
                'emojis': False
            },
            'twitter': {
                'max_length': 280,
                'style': 'engaging',
                'hashtags': True,
                'emojis': True
            }
        }

        template_config = platform_templates.get(platform, platform_templates['xiaohongshu'])

        # Generate suggestions based on insights
        if content['insights']:
            for insight in content['insights'][-5:]:  # Last 5 insights
                suggestion = {
                    'type': 'insight_sharing',
                    'source': insight,
                    'title': f"å…³äº{insight['content'][:20]}...çš„æ€è€ƒ",
                    'content': self._format_insight_post(insight, template_config),
                    'platform': platform or 'xiaohongshu',
                    'tags': self._extract_relevant_tags(insight['content'], analysis['themes'])
                }
                suggestions.append(suggestion)

        # Generate suggestions based on product experiences
        if content['product_experiences']:
            for product in content['product_experiences'][-3:]:  # Last 3 product experiences
                suggestion = {
                    'type': 'product_review',
                    'source': product,
                    'title': f"äº§å“ä½“éªŒåˆ†äº«: {product['content'][:30]}...",
                    'content': self._format_product_post(product, template_config),
                    'platform': platform or 'xiaohongshu',
                    'tags': ['äº§å“ä½“éªŒ', 'å·¥å…·æ¨è'] + self._extract_relevant_tags(product['content'], analysis['themes'])
                }
                suggestions.append(suggestion)

        # Generate suggestions based on learning progress
        if content['videos'] or content['readings']:
            learning_summary = self._create_learning_summary(content)
            suggestion = {
                'type': 'learning_summary',
                'source': {'videos': content['videos'], 'readings': content['readings']},
                'title': "è¿‘æœŸå­¦ä¹ æ€»ç»“åˆ†äº«",
                'content': self._format_learning_post(learning_summary, template_config),
                'platform': platform or 'xiaohongshu',
                'tags': ['å­¦ä¹ ç¬”è®°', 'æŠ€æœ¯åˆ†äº«'] + analysis['themes']['technology'][:3]
            }
            suggestions.append(suggestion)

        # Generate trend analysis suggestions
        if analysis['top_keywords']:
            suggestion = {
                'type': 'trend_analysis',
                'source': analysis,
                'title': "æŠ€æœ¯è¶‹åŠ¿è§‚å¯Ÿ",
                'content': self._format_trend_post(analysis, template_config),
                'platform': platform or 'xiaohongshu',
                'tags': ['è¶‹åŠ¿åˆ†æ', 'æŠ€æœ¯è§‚å¯Ÿ'] + [kw for kw, _ in analysis['top_keywords'][:3]]
            }
            suggestions.append(suggestion)

        return suggestions

    def _format_insight_post(self, insight: Dict[str, Any], config: Dict[str, Any]) -> str:
        """Format an insight into a social media post."""
        lines = []

        if config.get('emojis'):
            lines.append("ğŸ’¡ ä»Šæ—¥æ€è€ƒ")
            lines.append("")

        lines.append(insight['content'])

        if insight['details']:
            lines.append("")
            for detail in insight['details']:
                lines.append(f"â€¢ {detail}")

        if config.get('hashtags'):
            lines.append("")
            lines.append("#å­¦ä¹ æ€è€ƒ #æŠ€æœ¯æ´å¯Ÿ #äº§å“æ€ç»´")

        content = '\n'.join(lines)

        # Truncate if too long
        if len(content) > config['max_length']:
            content = content[:config['max_length'] - 3] + "..."

        return content

    def _format_product_post(self, product: Dict[str, Any], config: Dict[str, Any]) -> str:
        """Format a product experience into a social media post."""
        lines = []

        if config.get('emojis'):
            lines.append("ğŸ”§ äº§å“ä½“éªŒåˆ†äº«")
            lines.append("")

        lines.append(product['content'])

        if product['details']:
            lines.append("")
            lines.append("ä¸»è¦ç‰¹ç‚¹:")
            for i, detail in enumerate(product['details'][:5], 1):
                lines.append(f"{i}. {detail}")

        if config.get('hashtags'):
            lines.append("")
            lines.append("#äº§å“ä½“éªŒ #å·¥å…·æ¨è #æ•ˆç‡å·¥å…·")

        content = '\n'.join(lines)

        if len(content) > config['max_length']:
            content = content[:config['max_length'] - 3] + "..."

        return content

    def _format_learning_post(self, summary: Dict[str, Any], config: Dict[str, Any]) -> str:
        """Format learning summary into a social media post."""
        lines = []

        if config.get('emojis'):
            lines.append("ğŸ“š æœ¬å‘¨å­¦ä¹ æ€»ç»“")
            lines.append("")

        if summary['video_count'] > 0:
            lines.append(f"ğŸ¥ è§‚çœ‹äº† {summary['video_count']} ä¸ªå­¦ä¹ è§†é¢‘")

        if summary['reading_count'] > 0:
            lines.append(f"ğŸ“– å®Œæˆäº† {summary['reading_count']} é¡¹é˜…è¯»")

        if summary['key_topics']:
            lines.append("")
            lines.append("é‡ç‚¹å­¦ä¹ é¢†åŸŸ:")
            for topic in summary['key_topics'][:3]:
                lines.append(f"â€¢ {topic}")

        if config.get('hashtags'):
            lines.append("")
            lines.append("#å­¦ä¹ ç¬”è®° #æŠ€æœ¯å­¦ä¹  #æŒç»­å­¦ä¹ ")

        content = '\n'.join(lines)

        if len(content) > config['max_length']:
            content = content[:config['max_length'] - 3] + "..."

        return content

    def _format_trend_post(self, analysis: Dict[str, Any], config: Dict[str, Any]) -> str:
        """Format trend analysis into a social media post."""
        lines = []

        if config.get('emojis'):
            lines.append("ğŸ“Š æŠ€æœ¯è¶‹åŠ¿è§‚å¯Ÿ")
            lines.append("")

        lines.append("æ ¹æ®æœ€è¿‘çš„å­¦ä¹ å’Œè§‚å¯Ÿï¼Œä»¥ä¸‹æŠ€æœ¯å€¼å¾—å…³æ³¨:")
        lines.append("")

        for keyword, count in analysis['top_keywords'][:5]:
            lines.append(f"â€¢ {keyword} (å‡ºç° {count} æ¬¡)")

        if config.get('hashtags'):
            lines.append("")
            lines.append("#æŠ€æœ¯è¶‹åŠ¿ #è¡Œä¸šè§‚å¯Ÿ #æŠ€æœ¯åˆ†æ")

        content = '\n'.join(lines)

        if len(content) > config['max_length']:
            content = content[:config['max_length'] - 3] + "..."

        return content

    def _create_learning_summary(self, content: Dict[str, List[Any]]) -> Dict[str, Any]:
        """Create a summary of learning activities."""
        video_count = len(content['videos'])
        reading_count = len(content['readings'])

        # Extract key topics
        key_topics = []
        for video in content['videos']:
            if 'AI' in video['title'] or 'ai' in video['title'].lower():
                key_topics.append('äººå·¥æ™ºèƒ½')
            if 'Python' in video['title'] or 'python' in video['title'].lower():
                key_topics.append('Pythonç¼–ç¨‹')
            if 'äº§å“' in video['title']:
                key_topics.append('äº§å“è®¾è®¡')

        # Remove duplicates
        key_topics = list(set(key_topics))

        return {
            'video_count': video_count,
            'reading_count': reading_count,
            'key_topics': key_topics
        }

    def _extract_relevant_tags(self, text: str, themes: Dict[str, List[str]]) -> List[str]:
        """Extract relevant tags based on content and themes."""
        tags = []
        text_lower = text.lower()

        # Add theme-based tags
        for theme_name, keywords in themes.items():
            for keyword in keywords:
                if keyword in text_lower:
                    tags.append(keyword)

        return list(set(tags))[:5]  # Max 5 tags

    def run_ai_analysis(self, content: Dict[str, List[Any]]) -> Optional[str]:
        """Run AI-enhanced analysis if available."""
        try:
            # This would typically call an AI API
            # For now, return a placeholder
            return "AI åˆ†æåŠŸèƒ½éœ€è¦è”ç½‘ç¯å¢ƒæ‰èƒ½ä½¿ç”¨"
        except Exception as e:
            return f"AI åˆ†æå¤±è´¥: {e}"

    def format_output(self, content: Dict[str, List[Any]], analysis: Dict[str, Any],
                     suggestions: List[Dict[str, Any]], output_format: str,
                     verbose: bool = False) -> str:
        """Format the insight report in the specified format."""
        if output_format.lower() == 'json':
            data = {
                'content': content,
                'analysis': analysis,
                'suggestions': suggestions,
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_suggestions': len(suggestions)
                }
            }
            return json.dumps(data, ensure_ascii=False, indent=2)

        # Default to markdown format
        report_lines = []

        # Header
        report_lines.extend([
            "# ğŸ“ å†…å®¹åˆ›ä½œçµæ„ŸæŠ¥å‘Š",
            "",
            f"ğŸ“… **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}",
            f"ğŸ“Š **åˆ†æå†…å®¹**: {analysis['content_summary']['total_videos']} ä¸ªè§†é¢‘, {analysis['content_summary']['total_thoughts']} æ¡æ€è€ƒ, {analysis['content_summary']['total_insights']} ä¸ªæ´å¯Ÿ",
            "",
        ])

        # Content summary
        summary = analysis['content_summary']
        report_lines.extend([
            "## ğŸ“Š å†…å®¹æ¦‚è§ˆ",
            "",
            f"- ğŸ¥ å­¦ä¹ è§†é¢‘: {summary['total_videos']} ä¸ª",
            f"- ğŸ“š é˜…è¯»ææ–™: {summary['total_readings']} é¡¹",
            f"- ğŸ’­ æ·±åº¦æ€è€ƒ: {summary['total_thoughts']} æ¡",
            f"- ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ: {summary['total_insights']} ä¸ª",
            f"- ğŸ“ å­¦ä¹ è¾“å‡º: {summary['total_outputs']} é¡¹",
            f"- ğŸ”§ äº§å“ä½“éªŒ: {summary['total_products']} ä¸ª",
            "",
        ])

        # Theme analysis
        if analysis['top_keywords']:
            report_lines.extend([
                "## ğŸ¯ ä¸»é¢˜åˆ†æ",
                "",
                "### çƒ­é—¨å…³é”®è¯",
                "",
            ])
            for keyword, count in analysis['top_keywords']:
                report_lines.append(f"- **{keyword}**: {count} æ¬¡")
            report_lines.append("")

        # Content suggestions
        if suggestions:
            report_lines.extend([
                "## ğŸ’¡ å†…å®¹åˆ›ä½œå»ºè®®",
                "",
            ])

            for i, suggestion in enumerate(suggestions, 1):
                report_lines.extend([
                    f"### {i}. {suggestion['title']}",
                    "",
                    f"**ç±»å‹**: {suggestion['type']}",
                    f"**å¹³å°**: {suggestion['platform']}",
                    "",
                    "**å»ºè®®å†…å®¹**:",
                    "```",
                    suggestion['content'],
                    "```",
                    "",
                    f"**æ¨èæ ‡ç­¾**: {', '.join(suggestion['tags'][:5])}",
                    "",
                    "---",
                    "",
                ])

        # Detailed content if verbose
        if verbose:
            report_lines.extend([
                "## ğŸ“– è¯¦ç»†å†…å®¹åˆ†æ",
                "",
            ])

            if content['insights']:
                report_lines.extend([
                    "### ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ",
                    "",
                ])
                for insight in content['insights']:
                    report_lines.append(f"**{insight['date']}**: {insight['content']}")
                    for detail in insight['details']:
                        report_lines.append(f"  - {detail}")
                    report_lines.append("")

            if content['product_experiences']:
                report_lines.extend([
                    "### ğŸ”§ äº§å“ä½“éªŒ",
                    "",
                ])
                for product in content['product_experiences']:
                    report_lines.append(f"**{product['date']}**: {product['content']}")
                    for detail in product['details']:
                        report_lines.append(f"  - {detail}")
                    report_lines.append("")

        # Footer
        report_lines.extend([
            "---",
            "",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}*",
            "*åŸºäº claude_learning ç³»ç»Ÿå†…å®¹åˆ†æ*",
        ])

        return "\n".join(report_lines)


def main():
    """Main entry point for the insight script."""
    parser = argparse.ArgumentParser(
        description="ä»å­¦ä¹ è®°å½•ä¸­æå–çµæ„Ÿï¼Œç”Ÿæˆç¤¾äº¤åª’ä½“åˆ›ä½œå†…å®¹å»ºè®®",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--days', type=int, help='åˆ†ææœ€è¿‘Nå¤©çš„å†…å®¹')
    parser.add_argument('--weeks', type=int, help='åˆ†ææœ€è¿‘Nå‘¨çš„å†…å®¹')
    parser.add_argument('--all', action='store_true', help='åˆ†ææ‰€æœ‰å†å²å†…å®¹')
    parser.add_argument('--topic', help='èšç„¦ç‰¹å®šä¸»é¢˜åˆ†æ')
    parser.add_argument('--platform', choices=['xiaohongshu', 'weibo', 'linkedin', 'twitter'],
                       help='ä¸ºç‰¹å®šå¹³å°ä¼˜åŒ–')
    parser.add_argument('--format', choices=['markdown', 'json'],
                       default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--save', help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†åˆ†æè¿‡ç¨‹')
    parser.add_argument('--ai-analysis', action='store_true', help='å¯ç”¨AIæ·±åº¦åˆ†æ')

    args = parser.parse_args()

    analyzer = InsightAnalyzer()

    try:
        # Determine analysis range
        start_date, end_date = analyzer.get_analysis_date_range(
            days=args.days,
            weeks=args.weeks,
            all_content=args.all
        )

        print(f"ğŸ” åˆ†ææ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")

        # Collect content
        content = analyzer.collect_content_in_range(start_date, end_date, args.topic)

        # Analyze themes and trends
        analysis = analyzer.analyze_themes_and_trends(content)

        # Generate content suggestions
        suggestions = analyzer.generate_content_suggestions(content, analysis, args.platform)

        # Run AI analysis if requested
        if args.ai_analysis:
            ai_result = analyzer.run_ai_analysis(content)
            if ai_result:
                print(f"ğŸ¤– AI åˆ†æ: {ai_result}")

        # Format output
        formatted_report = analyzer.format_output(
            content, analysis, suggestions, args.format, args.verbose
        )

        # Save or print
        if args.save:
            output_path = Path(args.save)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(formatted_report)
            print(f"ğŸ“„ æ´å¯ŸæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        else:
            print(formatted_report)

        print(f"\nâœ¨ ç”Ÿæˆäº† {len(suggestions)} ä¸ªå†…å®¹åˆ›ä½œå»ºè®®")

        return 0

    except Exception as e:
        print(f"Error generating insight report: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())