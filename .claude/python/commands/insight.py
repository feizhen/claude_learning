#!/usr/bin/env python3
"""
Insight Command
ä»å­¦ä¹ è®°å½•ä¸­æå–ç¤¾äº¤åª’ä½“åˆ›ä½œçµæ„Ÿçš„Pythonå®ç°ã€‚

ä»å­¦ä¹ è®°å½•ä¸­æå–çµæ„Ÿï¼Œç”Ÿæˆé€‚ç”¨äºç¤¾äº¤åª’ä½“åˆ›ä½œçš„å†…å®¹å»ºè®®ã€‚
åˆ†æå­¦ä¹ å†…å®¹çš„ä¸»é¢˜è¶‹åŠ¿ã€äº§å“ä½“éªŒã€æŠ€æœ¯æ´å¯Ÿï¼Œä¸ºå†…å®¹åˆ›ä½œæä¾›ç´ æå’Œçµæ„Ÿã€‚
"""

import sys
import os
import argparse
import re
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime, timedelta

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥coreæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.date_utils import DateUtils
from core.file_utils import FileUtils
from core.content_parser import ContentParser
from core.templates import Templates


class InsightAnalyzer:
    """æ´å¯Ÿåˆ†æå™¨ç±»ã€‚"""

    def __init__(self):
        self.analyzed_files = 0
        self.content_sections = {
            'video': {},
            'newsletter': {},
            'braindump': {},
            'output': {},
            'review': {}
        }

    def extract_learning_content(self, days: Optional[int] = None, weeks: Optional[int] = None,
                               all_content: bool = False) -> None:
        """æå–å­¦ä¹ å†…å®¹ã€‚"""
        daily_files = FileUtils.find_daily_files()
        current_date = DateUtils.get_current_date()

        # ç¡®å®šåˆ†ææœŸé—´
        if all_content:
            cutoff_date = None
        elif days:
            cutoff_date = DateUtils.get_date_n_days_ago(days, current_date)
        elif weeks:
            cutoff_date = DateUtils.get_date_n_weeks_ago(weeks, current_date)
        else:
            cutoff_date = DateUtils.get_date_n_days_ago(7, current_date)  # é»˜è®¤7å¤©

        print("ğŸ“Š æ­£åœ¨æ‰«æå­¦ä¹ è®°å½•...", file=sys.stderr)

        for file_path in daily_files:
            # æå–æ–‡ä»¶æ—¥æœŸ
            filename = os.path.basename(file_path)
            file_date = DateUtils.parse_filename_date(filename)

            if file_date is None:
                continue

            # æ£€æŸ¥æ˜¯å¦åœ¨åˆ†ææœŸé—´å†…
            if cutoff_date and file_date < cutoff_date:
                continue

            self.analyzed_files += 1

            # è¯»å–æ–‡ä»¶å†…å®¹
            content = FileUtils.read_file(file_path)
            if not content:
                continue

            # æå–å„ä¸ªç« èŠ‚å†…å®¹
            sections = ContentParser.extract_all_sections(content)
            date_str = file_date.strftime("%Y-%m-%d")

            for section_name, section_content in sections.items():
                if section_content and section_name in self.content_sections:
                    self.content_sections[section_name][date_str] = section_content

        print(f"ğŸ“ˆ æ‰«æå®Œæˆ: åˆ†æäº† {self.analyzed_files} ä¸ªæ–‡ä»¶", file=sys.stderr)

    def analyze_content_themes(self) -> Dict[str, Dict[str, int]]:
        """åˆ†æå†…å®¹ä¸»é¢˜å’Œå…³é”®è¯ã€‚"""
        print("ğŸ” æ­£åœ¨åˆ†æå†…å®¹ä¸»é¢˜...", file=sys.stderr)

        # è·å–å…³é”®è¯åˆ†ç±»
        keyword_categories = Templates.content_analysis_keywords()
        keyword_counts = {}

        # åˆ†ææ¯ä¸ªå†…å®¹ç±»å‹
        for content_type, content_dict in self.content_sections.items():
            if not content_dict:
                continue

            print(f"åˆ†æ {content_type} å†…å®¹...", file=sys.stderr)

            for category_name, keywords_str in keyword_categories.items():
                keywords = keywords_str.split('|')

                for keyword in keywords:
                    count = 0
                    for date_str, content in content_dict.items():
                        count += len(re.findall(keyword, content, re.IGNORECASE))

                    if count > 0:
                        key = f"{keyword}:{count}:{content_type}"
                        keyword_counts[key] = count

        return keyword_counts

    def identify_valuable_content(self) -> Dict[str, List[str]]:
        """è¯†åˆ«é«˜ä»·å€¼å†…å®¹ç‰‡æ®µã€‚"""
        print("ğŸ’ æ­£åœ¨è¯†åˆ«é«˜ä»·å€¼å†…å®¹...", file=sys.stderr)

        valuable_content = {
            'insights': [],
            'achievements': [],
            'trends': []
        }

        # åˆ†æbraindumpä¸­çš„æ´å¯Ÿ
        for date_str, content in self.content_sections['braindump'].items():
            insight_keywords = ['æ€è€ƒ', 'æ´å¯Ÿ', 'å‘ç°', 'ä½“éªŒ', 'æ„Ÿå—', 'æ€»ç»“', 'å»ºè®®', 'æ¨è', 'ä¸é”™', 'å¾ˆæ£’', 'æœ‰è¶£', 'æƒŠå–œ']
            lines = content.split('\n')

            for line in lines:
                if any(keyword in line for keyword in insight_keywords):
                    if line.strip() and not line.strip().startswith('==='):
                        valuable_content['insights'].append(f"{date_str}: {line.strip()}")

        # åˆ†æoutputä¸­çš„æˆå°±
        for date_str, content in self.content_sections['output'].items():
            achievement_keywords = ['å®Œæˆ', 'å‘å¸ƒ', 'åˆ›å»º', 'å®ç°', 'æ­å»º', 'ä¸Šçº¿', 'demo', 'é¡¹ç›®', 'äº§å“']
            lines = content.split('\n')

            for line in lines:
                if any(keyword in line for keyword in achievement_keywords):
                    if line.strip() and not line.strip().startswith('==='):
                        valuable_content['achievements'].append(f"{date_str}: {line.strip()}")

        # åˆ†ænewsletterä¸­çš„è¶‹åŠ¿
        for date_str, content in self.content_sections['newsletter'].items():
            lines = content.split('\n')
            for line in lines:
                if len(line.strip()) > 10 and not line.strip().startswith('==='):
                    valuable_content['trends'].append(f"{date_str}: {line.strip()}")

        return valuable_content

    def generate_social_media_suggestions(self, keywords: Dict[str, int],
                                        valuable_content: Dict[str, List[str]],
                                        platform: Optional[str] = None) -> str:
        """ç”Ÿæˆç¤¾äº¤åª’ä½“å†…å®¹å»ºè®®ã€‚"""
        print("ğŸ“± æ­£åœ¨ç”Ÿæˆç¤¾äº¤åª’ä½“å†…å®¹å»ºè®®...", file=sys.stderr)

        suggestions = ["## ğŸš€ ç¤¾äº¤åª’ä½“å†…å®¹å»ºè®®\n\n"]

        # æ¨èè¯é¢˜æ ‡ç­¾
        if keywords:
            suggestions.append("### ğŸ·ï¸ æ¨èè¯é¢˜æ ‡ç­¾\n\n")
            # è·å–çƒ­é—¨å…³é”®è¯
            sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:10]

            for keyword_info, _ in sorted_keywords:
                keyword = keyword_info.split(':')[0]
                suggestions.append(f"- #{keyword}\n")

            suggestions.append("\n")

        # å¹³å°ä¸“å±å»ºè®®
        if platform:
            suggestions.append(f"### ğŸ“² {platform} ä¸“å±å»ºè®®\n\n")

            platform_tips = {
                "xiaohongshu": [
                    "**å°çº¢ä¹¦å†…å®¹ç‰¹ç‚¹:**",
                    "- æ ‡é¢˜è¦å¸å¼•çœ¼çƒï¼Œä½¿ç”¨æ•°å­—å’Œæƒ…ç»ªè¯æ±‡",
                    "- å¤šç”¨è¡¨æƒ…ç¬¦å·å¢åŠ è§†è§‰å¸å¼•åŠ›",
                    "- åˆ†äº«ä¸ªäººä½“éªŒå’ŒçœŸå®æ„Ÿå—",
                    "- åŒ…å«å®ç”¨çš„æ•™ç¨‹æˆ–å»ºè®®"
                ],
                "weibo": [
                    "**å¾®åšå†…å®¹ç‰¹ç‚¹:**",
                    "- ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹",
                    "- ç»“åˆæ—¶äº‹çƒ­ç‚¹",
                    "- ä½¿ç”¨ç›¸å…³çš„è¶…è¯æ ‡ç­¾",
                    "- é¼“åŠ±è½¬å‘å’Œäº’åŠ¨"
                ],
                "linkedin": [
                    "**LinkedIn å†…å®¹ç‰¹ç‚¹:**",
                    "- ä¸“ä¸šæ€§å¼ºï¼Œå±•ç¤ºä¸“ä¸šè§è§£",
                    "- åˆ†äº«èŒä¸šç»éªŒå’Œå­¦ä¹ å¿ƒå¾—",
                    "- è‹±æ–‡å†…å®¹ä¸ºä¸»",
                    "- é€‚åˆæŠ€æœ¯æ·±åº¦åˆ†äº«"
                ],
                "twitter": [
                    "**Twitter å†…å®¹ç‰¹ç‚¹:**",
                    "- ç®€çŸ­ç²¾ç‚¼ï¼Œä¸€é’ˆè§è¡€",
                    "- ä½¿ç”¨ thread å±•å¼€å¤æ‚è¯é¢˜",
                    "- åŠæ—¶æ€§å¼ºï¼Œè¿½æ±‚viralä¼ æ’­",
                    "- å¤šä½¿ç”¨ç›¸å…³ hashtag"
                ]
            }

            if platform in platform_tips:
                for tip in platform_tips[platform]:
                    suggestions.append(f"{tip}\n")
                suggestions.append("\n")

        # å†…å®¹æ ¼å¼å»ºè®®
        suggestions.extend([
            "### ğŸ“ å†…å®¹æ ¼å¼å»ºè®®\n\n",
            "**æ¨èå†…å®¹ç±»å‹:**\n",
            "- ğŸ“Š å­¦ä¹ æ€»ç»“ - å°†æœ¬å‘¨/æœ¬æœˆå­¦ä¹ å†…å®¹åˆ¶ä½œæˆå›¾è¡¨\n",
            "- ğŸ”§ å·¥å…·åˆ†äº« - ä»‹ç»ä½¿ç”¨è¿‡çš„ä¼˜ç§€å·¥å…·å’Œä½“éªŒ\n",
            "- ğŸ’¡ æ€è€ƒæ„Ÿæ‚Ÿ - åˆ†äº«å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ´å¯Ÿå’Œæ€è€ƒ\n",
            "- ğŸ¯ æˆæœå±•ç¤º - å±•ç¤ºå­¦ä¹ æˆæœå’Œé¡¹ç›®è¿›å±•\n",
            "- ğŸ“š èµ„æºæ¨è - æ¨èä¼˜è´¨çš„å­¦ä¹ èµ„æºå’Œæ–‡ç« \n\n"
        ])

        # å‘å¸ƒæ—¶æœºå»ºè®®
        suggestions.extend([
            "### â° å‘å¸ƒæ—¶æœºå»ºè®®\n\n",
            "**æœ€ä½³å‘å¸ƒæ—¶é—´:**\n",
            "- å·¥ä½œæ—¥æ—©ä¸Š 8-9 ç‚¹ï¼ˆä¸Šç­è·¯ä¸Šï¼‰\n",
            "- åˆä¼‘æ—¶é—´ 12-13 ç‚¹\n",
            "- æ™šä¸Š 20-22 ç‚¹ï¼ˆä¼‘æ¯æ—¶é—´ï¼‰\n",
            "- å‘¨æœ«ä¸‹åˆ 14-17 ç‚¹\n\n"
        ])

        return ''.join(suggestions)

    def generate_platform_content_examples(self, valuable_content: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆå¹³å°ä¸“å±å†…å®¹ç¤ºä¾‹ã€‚"""
        print("ğŸ“± æ­£åœ¨ç”Ÿæˆå¹³å°ä¸“å±å†…å®¹ç¤ºä¾‹...", file=sys.stderr)

        examples = ["## ğŸ“² å¹³å°ä¸“å±å†…å®¹ç¤ºä¾‹\n\n"]

        # è·å–ä¸€ä¸ªç¤ºä¾‹æ´å¯Ÿ
        sample_insight = "ä»Šå¤©ä½“éªŒäº†ä¸€ä¸ªAIå·¥å…·ï¼Œå‘ç°å®ƒçš„ç”¨æˆ·ä½“éªŒè®¾è®¡éå¸¸å‡ºè‰²"
        if valuable_content['insights']:
            sample_line = valuable_content['insights'][0]
            if ':' in sample_line:
                sample_insight = sample_line.split(':', 1)[1].strip()

        # å°çº¢ä¹¦ç‰ˆæœ¬
        examples.extend([
            "### ğŸƒ å°çº¢ä¹¦ç‰ˆæœ¬\n\n",
            "```\n",
            "ğŸš€ åˆå‘ç°ä¸€ä¸ªå®è—AIå·¥å…·ï¼\n\n",
            "å§å¦¹ä»¬ï¼Œä»Šå¤©å¿…é¡»è¦åˆ†äº«è¿™ä¸ªå‘ç°ï¼\n\n",
            f"âœ¨ {sample_insight}\n\n",
            "ğŸ”¥ æœ€è®©æˆ‘æƒŠå–œçš„å‡ ä¸ªç‚¹ï¼š\n",
            "1ï¸âƒ£ ç•Œé¢è®¾è®¡è¶…çº§ç®€æ´å¥½çœ‹\n",
            "2ï¸âƒ£ æ–°æ‰‹å¼•å¯¼åšå¾—ç‰¹åˆ«è´´å¿ƒ\n",
            "3ï¸âƒ£ åŠŸèƒ½å¼ºå¤§ä½†ä¸ä¼šè®©äººè§‰å¾—å¤æ‚\n\n",
            "ä½ ä»¬è¿˜æœ‰ä»€ä¹ˆå¥½ç”¨çš„AIå·¥å…·æ¨èå—ï¼Ÿè¯„è®ºåŒºè§ğŸ‘‡\n\n",
            "#AIå·¥å…· #æ•ˆç‡ç¥å™¨ #äº§å“ä½“éªŒ #ç§‘æŠ€åˆ†äº« #å­¦ä¹ ç¬”è®°\n",
            "```\n\n"
        ])

        # å¾®åšç‰ˆæœ¬
        examples.extend([
            "### ğŸ¦ å¾®åšç‰ˆæœ¬\n\n",
            "```\n",
            f"ğŸ’¡ äº§å“è§‚å¯Ÿï¼š{sample_insight}\n\n",
            "ä½œä¸ºä¸€ä¸ªäº§å“çˆ±å¥½è€…ï¼Œä»Šå¤©æ·±åº¦ä½“éªŒäº†è¿™ä¸ªAIå·¥å…·ï¼Œå‡ ä¸ªå€¼å¾—æ€è€ƒçš„ç‚¹ï¼š\n\n",
            "1. ç”¨æˆ·å¼•å¯¼çš„åŠ›é‡ï¼šå¥½çš„å¼•å¯¼èƒ½è®©å¤æ‚åŠŸèƒ½å˜å¾—æ˜“æ‡‚\n",
            "2. è®¾è®¡çš„æ¸©åº¦æ„Ÿï¼šæŠ€æœ¯äº§å“ä¹Ÿéœ€è¦äººæ–‡å…³æ€€\n",
            "3. åŠŸèƒ½ä¸ç®€æ´çš„å¹³è¡¡ï¼šå…‹åˆ¶æ¯”å †ç Œæ›´éœ€è¦æ™ºæ…§\n\n",
            "ä½ è§‰å¾—ä¸€ä¸ªAIäº§å“æœ€é‡è¦çš„æ˜¯ä»€ä¹ˆï¼ŸæŠ€æœ¯è¿˜æ˜¯ä½“éªŒï¼Ÿ\n\n",
            "#äº§å“æ€è€ƒ #AIå·¥å…· #ç”¨æˆ·ä½“éªŒ #äº§å“è®¾è®¡ #ç§‘æŠ€è§‚å¯Ÿ\n",
            "```\n\n"
        ])

        # LinkedInç‰ˆæœ¬
        examples.extend([
            "### ğŸ’¼ LinkedInç‰ˆæœ¬\n\n",
            "```\n",
            "ğŸ¯ Product Insights: What Makes AI Tools Truly User-Friendly?\n\n",
            "After spending time with a new AI tool today, I was struck by how thoughtful user onboarding can transform the entire experience.\n\n",
            "Key observations:\n\n",
            "ğŸ”¸ Progressive Disclosure: Complex features introduced gradually\n",
            "ğŸ”¸ Contextual Guidance: Help appears exactly when and where needed\n",
            "ğŸ”¸ Emotional Design: The interface feels approachable, not intimidating\n\n",
            "For product teams building in this space:\n",
            "â€¢ Invest heavily in onboarding design\n",
            "â€¢ Test with real users, not just tech-savvy early adopters\n",
            "â€¢ Remember that complexity should be hidden, not eliminated\n\n",
            "What's your experience with AI tool adoption in your organization?\n\n",
            "#ProductManagement #AITools #UserExperience #TechInnovation #ProductDesign\n",
            "```\n\n"
        ])

        return ''.join(examples)

    def create_content_templates(self) -> str:
        """åˆ›å»ºå†…å®¹åˆ›ä½œæ¨¡æ¿ã€‚"""
        print("ğŸ“‹ æ­£åœ¨åˆ›å»ºå†…å®¹æ¨¡æ¿...", file=sys.stderr)

        templates = ["## ğŸ“„ å†…å®¹åˆ›ä½œæ¨¡æ¿\n\n"]

        # å­¦ä¹ æ€»ç»“æ¨¡æ¿
        templates.extend([
            "### ğŸ“Š å­¦ä¹ æ€»ç»“æ¨¡æ¿\n\n",
            "```\n",
            "ğŸ“š è¿™å‘¨çš„å­¦ä¹ æ”¶è· #å­¦ä¹ è®°å½• #AIå­¦ä¹ \n\n",
            "æœ¬å‘¨é‡ç‚¹å­¦ä¹ äº†ï¼š\n",
            "ğŸ”¸ [æŠ€æœ¯/å·¥å…·åç§°] - [ç®€çŸ­æè¿°]\n",
            "ğŸ”¸ [é‡è¦æ¦‚å¿µ] - [ä¸ªäººç†è§£]\n",
            "ğŸ”¸ [å®è·µé¡¹ç›®] - [å…·ä½“æˆæœ]\n\n",
            "ğŸ’¡ æœ€å¤§çš„æ”¶è·ï¼š\n",
            "[å†™å‡ºæœ€æœ‰ä»·å€¼çš„æ´å¯Ÿæˆ–ä½“éªŒ]\n\n",
            "ğŸ¯ ä¸‹å‘¨è®¡åˆ’ï¼š\n",
            "[ç®€è¿°ä¸‹å‘¨å­¦ä¹ é‡ç‚¹]\n\n",
            "#æŒç»­å­¦ä¹  #æŠ€æœ¯æˆé•¿ #ä¸ªäººå‘å±•\n",
            "```\n\n"
        ])

        # å·¥å…·ä½“éªŒæ¨¡æ¿
        templates.extend([
            "### ğŸ”§ å·¥å…·ä½“éªŒæ¨¡æ¿\n\n",
            "```\n",
            "ğŸ› ï¸ [å·¥å…·åç§°]ä½“éªŒåˆ†äº« #å·¥å…·æ¨è\n\n",
            "âœ¨ äº®ç‚¹åŠŸèƒ½ï¼š\n",
            "â€¢ [åŠŸèƒ½1] - [å…·ä½“ä½“éªŒ]\n",
            "â€¢ [åŠŸèƒ½2] - [ä½¿ç”¨æ„Ÿå—]\n",
            "â€¢ [åŠŸèƒ½3] - [å®é™…ä»·å€¼]\n\n",
            "ğŸ‘ æ¨èæŒ‡æ•°ï¼šâ­â­â­â­â­\n",
            "ğŸ’° ä»˜è´¹æƒ…å†µï¼š[å…è´¹/ä»˜è´¹]\n",
            "ğŸ¯ é€‚ç”¨åœºæ™¯ï¼š[å…·ä½“åº”ç”¨åœºæ™¯]\n\n",
            "æ€»ç»“ï¼š[ä¸€å¥è¯æ€»ç»“å·¥å…·ä»·å€¼]\n\n",
            "#æ•ˆç‡å·¥å…· #äº§å“ä½“éªŒ #ç”Ÿäº§åŠ›\n",
            "```\n\n"
        ])

        # æ€è€ƒåˆ†äº«æ¨¡æ¿
        templates.extend([
            "### ğŸ’¡ æ€è€ƒåˆ†äº«æ¨¡æ¿\n\n",
            "```\n",
            "ğŸ¤” å…³äº[è¯é¢˜]çš„ä¸€äº›æ€è€ƒ\n\n",
            "æœ€è¿‘åœ¨å­¦ä¹ /ä½¿ç”¨[å…·ä½“å†…å®¹]çš„è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼š\n\n",
            "[æ ¸å¿ƒè§‚ç‚¹æˆ–æ´å¯Ÿ]\n\n",
            "è¿™è®©æˆ‘æƒ³åˆ°ï¼š\n",
            "â€¢ [å»¶ä¼¸æ€è€ƒ1]\n",
            "â€¢ [å»¶ä¼¸æ€è€ƒ2]\n",
            "â€¢ [å®é™…åº”ç”¨]\n\n",
            "ä½ ä»¬æ€ä¹ˆçœ‹ï¼Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«ä½ çš„æƒ³æ³•ğŸ‘‡\n\n",
            "#æ·±åº¦æ€è€ƒ #è¡Œä¸šæ´å¯Ÿ #äº’åŠ¨è®¨è®º\n",
            "```\n\n"
        ])

        return ''.join(templates)

    def generate_keyword_analysis(self, keywords: Dict[str, int]) -> str:
        """ç”Ÿæˆå…³é”®è¯åˆ†æã€‚"""
        if not keywords:
            return ""

        analysis = ["## ğŸ“Š å…³é”®è¯åˆ†æ\n\n"]

        # çƒ­é—¨å…³é”®è¯æ’è¡Œ
        analysis.append("### ğŸ”¥ çƒ­é—¨å…³é”®è¯æ’è¡Œ\n\n")

        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:15]
        for keyword_info, _ in sorted_keywords:
            parts = keyword_info.split(':')
            if len(parts) >= 3:
                keyword, count, source = parts[0], parts[1], parts[2]
                analysis.append(f"- **{keyword}** (å‡ºç° {count} æ¬¡ï¼Œæ¥æº: {source})\n")

        analysis.append("\n")

        # åˆ†ç±»å…³é”®è¯ç»Ÿè®¡
        analysis.append("### ğŸ“‚ åˆ†ç±»å…³é”®è¯ç»Ÿè®¡\n\n")

        for category in ['video', 'newsletter', 'braindump', 'output', 'review']:
            category_keywords = [k for k in sorted_keywords if k[0].endswith(f':{category}')]

            if category_keywords:
                analysis.append(f"**{category} ç›¸å…³å…³é”®è¯ ({len(category_keywords)} ä¸ª):**\n")
                for keyword_info, _ in category_keywords[:5]:
                    parts = keyword_info.split(':')
                    if len(parts) >= 2:
                        keyword, count = parts[0], parts[1]
                        analysis.append(f"- {keyword} ({count})\n")
                analysis.append("\n")

        return ''.join(analysis)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(description='Generate insight report from learning records')
    parser.add_argument('--days', type=int, help='Analyze content from last N days')
    parser.add_argument('--weeks', type=int, help='Analyze content from last N weeks')
    parser.add_argument('--all', action='store_true', help='Analyze all historical content')
    parser.add_argument('--topic', help='Focus on specific topic analysis')
    parser.add_argument('--platform', choices=['xiaohongshu', 'weibo', 'linkedin', 'twitter'],
                       help='Optimize for specific platform')
    parser.add_argument('--format', choices=['markdown', 'json', 'html'], default='markdown',
                       help='Output format')
    parser.add_argument('--save', help='Save result to file')
    parser.add_argument('--verbose', action='store_true', help='Show detailed analysis process')
    parser.add_argument('--ai-analysis', action='store_true', help='Enable AI deep analysis')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œinsightå‘½ä»¤é€»è¾‘ã€‚"""
    try:
        args = parse_arguments()

        print("ğŸ¯ å¯åŠ¨å†…å®¹æ´å¯Ÿåˆ†æ...", file=sys.stderr)

        # åˆ›å»ºåˆ†æå™¨
        analyzer = InsightAnalyzer()

        # æå–å­¦ä¹ å†…å®¹
        analyzer.extract_learning_content(
            days=args.days,
            weeks=args.weeks,
            all_content=args.all
        )

        if analyzer.analyzed_files == 0:
            print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­¦ä¹ è®°å½•æ–‡ä»¶ã€‚")
            return

        # åˆ†æä¸»é¢˜å’Œå…³é”®è¯
        keywords = analyzer.analyze_content_themes()

        # è¯†åˆ«é«˜ä»·å€¼å†…å®¹
        valuable_content = analyzer.identify_valuable_content()

        # ç”Ÿæˆç¤¾äº¤åª’ä½“å»ºè®®
        social_suggestions = analyzer.generate_social_media_suggestions(
            keywords, valuable_content, args.platform
        )

        # ç”Ÿæˆå†…å®¹æ¨¡æ¿
        content_templates = analyzer.create_content_templates()

        # ç”Ÿæˆå¹³å°ä¸“å±ç¤ºä¾‹
        platform_examples = analyzer.generate_platform_content_examples(valuable_content)

        # ç”Ÿæˆå…³é”®è¯åˆ†æ
        keyword_analysis = analyzer.generate_keyword_analysis(keywords)

        # ç”ŸæˆæŠ¥å‘Š
        current_date = DateUtils.format_chinese_date(DateUtils.get_current_date())

        # ç¡®å®šåˆ†æèŒƒå›´æè¿°
        if args.all:
            analysis_range = "æ‰€æœ‰å†å²å†…å®¹"
        elif args.days:
            analysis_range = f"æœ€è¿‘ {args.days} å¤©"
        elif args.weeks:
            analysis_range = f"æœ€è¿‘ {args.weeks} å‘¨"
        else:
            analysis_range = "æœ€è¿‘ 7 å¤©"

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_content = [
            f"# ğŸ¯ Insight Report - {current_date}\n\n",
            "## ğŸ“ˆ åˆ†ææ‘˜è¦\n\n",
            f"- **åˆ†æèŒƒå›´**: {analysis_range}\n",
            f"- **åˆ†ææ–‡ä»¶æ•°**: {analyzer.analyzed_files} ä¸ªå­¦ä¹ è®°å½•\n",
            f"- **ä¸»é¢˜ç„¦ç‚¹**: {args.topic if args.topic else 'å…¨éƒ¨ä¸»é¢˜'}\n",
            f"- **ç›®æ ‡å¹³å°**: {args.platform if args.platform else 'é€šç”¨å¹³å°'}\n\n"
        ]

        # AIåˆ†æå ä½ç¬¦
        if args.ai_analysis:
            report_content.extend([
                "## ğŸ§  AI æ·±åº¦åˆ†ææŠ¥å‘Š\n\n",
                "### âš ï¸ AIåˆ†æè¯´æ˜\n\n",
                "AIæ·±åº¦åˆ†æåŠŸèƒ½éœ€è¦åœ¨Claude Codeç¯å¢ƒä¸­ä½¿ç”¨ã€‚è¯·åœ¨Claude Codeä¸­è¿è¡Œæ­¤å‘½ä»¤ä»¥è·å¾—ï¼š\n\n",
                "- ğŸ¯ å†…å®¹ä»·å€¼è¯„ä¼°å’Œä¼ æ’­æ½œåŠ›åˆ†æ\n",
                "- ğŸ‘¥ ç›®æ ‡å—ä¼—ç”»åƒå’Œå¹³å°æ¨è\n",
                "- âœï¸ å¤šé£æ ¼æ–‡æ¡ˆå»ºè®®å’Œæ ‡é¢˜ä¼˜åŒ–\n",
                "- ğŸ–¼ï¸ é…å›¾æ–¹å‘å’Œè§†è§‰å‘ˆç°å»ºè®®\n",
                "- â° æœ€ä½³å‘å¸ƒæ—¶æœºå’Œç­–ç•¥å»ºè®®\n\n"
            ])

        # æ·»åŠ é«˜ä»·å€¼å†…å®¹ç‰‡æ®µ
        if valuable_content['insights'] or valuable_content['achievements']:
            report_content.append("## ğŸŒŸ é«˜ä»·å€¼å†…å®¹ç‰‡æ®µ\n\n")

            if valuable_content['insights']:
                report_content.append("### ğŸ’¡ æ·±åº¦æ€è€ƒä¸æ´å¯Ÿ\n\n")
                for insight in valuable_content['insights'][:10]:
                    if ':' in insight:
                        date_str, content = insight.split(':', 1)
                        report_content.append(f"- **{date_str}**: {content.strip()}\n")
                report_content.append("\n")

            if valuable_content['achievements']:
                report_content.append("### ğŸ¯ å­¦ä¹ æˆæœä¸äº§å‡º\n\n")
                for achievement in valuable_content['achievements'][:8]:
                    if ':' in achievement:
                        date_str, content = achievement.split(':', 1)
                        report_content.append(f"- **{date_str}**: {content.strip()}\n")
                report_content.append("\n")

        # æ·»åŠ å…¶ä»–éƒ¨åˆ†
        report_content.append(social_suggestions)
        report_content.append(content_templates)
        report_content.append(platform_examples)
        report_content.append(keyword_analysis)

        # æ·»åŠ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
        report_content.extend([
            "## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®\n\n",
            "### ğŸ“… å†…å®¹åˆ›ä½œè®¡åˆ’\n\n",
            "- **æœ¬å‘¨é‡ç‚¹**: é€‰æ‹©1-2ä¸ªé«˜ä»·å€¼å†…å®¹ç‰‡æ®µï¼Œåˆ¶ä½œæˆç¤¾äº¤åª’ä½“å¸–å­\n",
            "- **å†…å®¹æ—¥ç¨‹**: \n",
            "  - å‘¨ä¸€ï¼šå‘å¸ƒå­¦ä¹ æ€»ç»“ç±»å†…å®¹\n",
            "  - å‘¨ä¸‰ï¼šåˆ†äº«å·¥å…·ä½“éªŒæˆ–äº§å“è¯„æµ‹\n",
            "  - å‘¨äº”ï¼šå‘å¸ƒæ€è€ƒæ´å¯Ÿç±»å†…å®¹\n",
            "- **äº’åŠ¨ç­–ç•¥**: åœ¨å¸–å­ä¸­åŠ å…¥é—®é¢˜ï¼Œé¼“åŠ±è¯»è€…è¯„è®ºå’Œåˆ†äº«\n",
            "- **å†…å®¹ä¼˜åŒ–**: æ ¹æ®ä¸åŒå¹³å°ç‰¹ç‚¹è°ƒæ•´å†…å®¹æ ¼å¼å’Œé•¿åº¦\n\n",
            "### ğŸ”„ æŒç»­æ”¹è¿›\n\n",
            "- å®šæœŸä½¿ç”¨ `/insight` å‘½ä»¤åˆ†æå­¦ä¹ å†…å®¹ï¼ˆå»ºè®®æ¯å‘¨ä¸€æ¬¡ï¼‰\n",
            "- è·Ÿè¸ªå‘å¸ƒå†…å®¹çš„åé¦ˆå’Œäº’åŠ¨æ•°æ®\n",
            "- æ ¹æ®å—ä¼—ååº”è°ƒæ•´å†…å®¹ä¸»é¢˜å’Œé£æ ¼\n",
            "- å»ºç«‹å†…å®¹ç´ æåº“ï¼Œç§¯ç´¯å¯å¤ç”¨çš„è§‚ç‚¹å’Œé‡‘å¥\n\n",
            "---\n",
            "*æœ¬æŠ¥å‘Šç”± /insight å‘½ä»¤è‡ªåŠ¨ç”Ÿæˆï¼ŒåŸºäºæ‚¨çš„å­¦ä¹ è®°å½•åˆ†æ*\n"
        ])

        final_report = ''.join(report_content)

        # è¾“å‡ºæŠ¥å‘Š
        if args.save:
            FileUtils.write_file(args.save, final_report)
            print(f"âœ… æ´å¯ŸæŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.save}", file=sys.stderr)
        else:
            print(final_report)

        print("ğŸ‰ åˆ†æå®Œæˆï¼", file=sys.stderr)

    except Exception as e:
        print(f"Error generating insight report: {str(e)}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()